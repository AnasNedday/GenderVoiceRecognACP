# GenderVoiceRecognitionPCA

## Principal component analysis (PCA) is a statistical procedure that is used to reduce the dimensionality. It uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. It is often used as a dimensionality reduction technique.

### Steps Involved in the PCA : 

##### Step 1: Standardize the dataset.

#####  Step 2: Calculate the covariance matrix for the features in the dataset.

#### Step 3: Calculate the eigenvalues and eigenvectors for the covariance matrix.

##### Step 4: Sort eigenvalues and their corresponding eigenvectors.

##### Step 5: Pick k eigenvalues and form a matrix of eigenvectors.

##### Step 6: Transform the original matrix.

## Gender recognition through voice and speech analysis:

This database was created to identify a male or female voice, based on the acoustic properties of voice and speech.
The dataset includes 3,168 recorded voice samples collected from male and female speakers.
Our data set starts with 20 variables.

![Python](Python.png) ![Jupyter](Jupyter.png)
### Applied algorithm:
##### Steps to follow :
Correlation Matrix <br>
Data centering <br>
Data reduction <br>
Covariance-variance matrix <br>
Calculation of eigenvalues and vectors <br>
Calculation of sum of eigenvalues <br>
Calculation of inertia percentages and cumulative inertia percentages <br>
Choice of variables to keep <br>

# XGBoost for Regression

XGBoost is an optimized distributed gradient boosting library designed for efficient and scalable training of machine learning models. It is an ensemble learning method that combines the predictions of multiple weak models to produce a stronger prediction. XGBoost stands for “Extreme Gradient Boosting” and it has become one of the most popular and widely used machine learning algorithms due to its ability to handle large datasets and its ability to achieve state-of-the-art performance in many machine learning tasks such as classification and regression.

One of the key features of XGBoost is its efficient handling of missing values, which allows it to handle real-world data with missing values without requiring significant pre-processing. Additionally, XGBoost has built-in support for parallel processing, making it possible to train models on large datasets in a reasonable amount of time.

XGBoost can be used in a variety of applications, including Kaggle competitions, recommendation systems, and click-through rate prediction, among others. It is also highly customizable and allows for fine-tuning of various model parameters to optimize performance.

XgBoost stands for Extreme Gradient Boosting, which was proposed by the researchers at the University of Washington. It is a library written in C++ which optimizes the training for Gradient Boosting.
